{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "01.train_deep_and_wide.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyP5jWxE0sNcFiVvbpmCOuu+"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# SETUP the colab Env."
   ],
   "metadata": {
    "id": "fdHWR9Ay9nEV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # import drive from google colab\n",
    "\n",
    "ROOT = \"/content/drive\"     # default location for the drive\n",
    "print(ROOT)                 # print content of ROOT (Optional)\n",
    "\n",
    "drive.mount(ROOT)           # we mount the google drive at /content/drive"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P8nBjpS87OB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155265060,
     "user_tz": -120,
     "elapsed": 20418,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "005d95a3-4fce-4552-a793-f8f821235bf1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive\n",
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Clone github repository setup\n",
    "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
    "from os.path import join  \n",
    "\n",
    "# path to your project on Google Drive\n",
    "MY_GOOGLE_DRIVE_PATH = 'MyDrive/Colab Notebooks/'  \n",
    "# Replace with your github repository\n",
    "GIT_REPOSITORY = \"promo-recommendation\" \n",
    "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
    "\n",
    "# It's good to print out the value if you are not sure \n",
    "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
    "\n",
    "# In case we haven't created the folder already; we will create a folder in the project path \n",
    "# !mkdir \"{PROJECT_PATH}\"    \n",
    "\n",
    "GIT_PATH = \"https://\" + \"@github.com/\" + \"fellowship/\" + GIT_REPOSITORY + \".git\"\n",
    "print(\"GIT_PATH: \", GIT_PATH)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ka7vN7zu9Mza",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155273262,
     "user_tz": -120,
     "elapsed": 208,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "89aa6df6-ffe6-4606-88da-e25f80ede02f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PROJECT_PATH:  /content/drive/MyDrive/Colab Notebooks/\n",
      "GIT_PATH:  https://@github.com/fellowship/promo-recommendation.git\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd \"{PROJECT_PATH}{\"promo-recommendation/\"}\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Yxv8N149P_l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155279141,
     "user_tz": -120,
     "elapsed": 207,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "fac60dc3-5a7e-489e-cd7c-836a384ebdd1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/promo-recommendation\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git branch -av"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZCYkZB-9S0H",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155288366,
     "user_tz": -120,
     "elapsed": 5024,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "4aff1056-a93b-4e25-81ea-bf4e5298426c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "* \u001B[32mdeep_and_wide                         \u001B[m 277ba41 directory change\n",
      "  main                                  \u001B[m 401931c Merge pull request #4 from fellowship/data_generation_refine\n",
      "  \u001B[31mremotes/origin/HEAD                   \u001B[m -> origin/main\n",
      "  \u001B[31mremotes/origin/cohort_var_xgb         \u001B[m 025362d initial implementation of cohort variance analysis\n",
      "  \u001B[31mremotes/origin/deep_and_wide          \u001B[m 277ba41 directory change\n",
      "  \u001B[31mremotes/origin/initial_data_generation\u001B[m 5a736b5 add script and plots for example data\n",
      "  \u001B[31mremotes/origin/main                   \u001B[m 401931c Merge pull request #4 from fellowship/data_generation_refine\n",
      "  \u001B[31mremotes/origin/setup                  \u001B[m a15a8c2 update readme\n",
      "  \u001B[31mremotes/origin/uneven_cohort_fix      \u001B[m 85cb0fe fix a sampling frequency bug where number of elements to modify is greater than total number of elements\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git branch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lot7G9Z39dPi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155302604,
     "user_tz": -120,
     "elapsed": 246,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "481f9ed9-38db-4e65-cd88-7aac169dc686",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "* \u001B[32mdeep_and_wide\u001B[m\n",
      "  main\u001B[m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python ./00.generate_data.py"
   ],
   "metadata": {
    "id": "VcuUGD7v9hLY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtcr1aOp5WO9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import importlib\n",
    "import os\n",
    "from routine.utilities import generate_CSV, df_to_dataloader, generate_feature_columns\n",
    "from routine.data_generation import generate_data\n",
    "from routine.models import build_wide_model, build_deep_model, build_wide_and_deep_model, \\\n",
    "    build_bayesian_model, evaluate_bandit\n",
    "from os.path import exists\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_regenerate = False\n",
    "if data_regenerate:\n",
    "    obs_df, user_df, camp_df = generate_data(\n",
    "        num_users=100,\n",
    "        num_campaigns=10,\n",
    "        samples_per_campaign=100,\n",
    "        num_cohort=10,\n",
    "        cohort_variances=np.linspace(0.05, 0.6, 10),\n",
    "        fh_cohort=True,\n",
    "        response_sig_a=10,\n",
    "        even_cohort=True,\n",
    "        cross_response=True\n",
    "    )\n",
    "else:\n",
    "    obs_df = pd.read_csv('observation_odd.csv')\n",
    "\n",
    "\n",
    "INPUT_DATA_PATH = './deep_and_wide/NN_Inputs/input_data'\n",
    "if not os.path.isdir(INPUT_DATA_PATH):\n",
    "    os.makedirs(INPUT_DATA_PATH)"
   ],
   "metadata": {
    "id": "M_5XXlBi50Wb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jy8XStUG58yd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155354136,
     "user_tz": -120,
     "elapsed": 1965,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "7967f944-505c-4a30-ca8a-309a38840df5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] The training dataframe has 601 instances from indices 0 to 599\n",
      "[INFO] The validation dataframe has 201 instances from indices 600 to 799\n",
      "[INFO] The training dataframe has 200 instances from indices 800 to 1000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_path = INPUT_DATA_PATH + \"/train.csv\"\n",
    "val_path = INPUT_DATA_PATH + \"/val.csv\"\n",
    "test_path = INPUT_DATA_PATH + \"/test.csv\"\n",
    "re_create = True\n",
    "if re_create:\n",
    "    generate_CSV(obs_df,\n",
    "                 train_path,\n",
    "                 val_path,\n",
    "                 test_path,\n",
    "                 verbose=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM2zBp9w6CFI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155374172,
     "user_tz": -120,
     "elapsed": 543,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "2145d586-6106-4805-d1af-f5eaf4142a9b",
    "pycharm": {
     "name": "#%% Creating the training, validation and testing data for the model\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] Train dataloader:\n",
      "<BatchDataset element_spec=({'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'camp_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'cohort': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None)}, TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>\n",
      "[INFO] Val dataloader:\n",
      "<BatchDataset element_spec=({'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'camp_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'cohort': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None)}, TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>\n",
      "[INFO] Test dataloader:\n",
      "<BatchDataset element_spec=({'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'camp_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'cohort': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None)}, TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "GzGLSlks6GkX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 100\n",
    "n_epochs = 100\n",
    "feature_columns = [\"user_id\", \"camp_id\", \"cohort\",\n",
    "                   \"user_f0\", \"user_f1\", \"user_fh\",\n",
    "                   \"camp_f0\", \"camp_f1\", \"camp_fh\"]\n",
    "target_column = \"response\"\n",
    "\n",
    "train_dl = df_to_dataloader(train_path,\n",
    "                            feature_columns,\n",
    "                            target_column,\n",
    "                            batch_size=batch_size)\n",
    "val_dl = df_to_dataloader(val_path,\n",
    "                          feature_columns,\n",
    "                          target_column,\n",
    "                          batch_size=batch_size)\n",
    "test_dl = df_to_dataloader(test_path,\n",
    "                           feature_columns,\n",
    "                           target_column,\n",
    "                           shuffle=False,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "print(\"[INFO] Train dataloader:\")\n",
    "pprint(train_dl)\n",
    "print(\"[INFO] Val dataloader:\")\n",
    "pprint(val_dl)\n",
    "print(\"[INFO] Test dataloader:\")\n",
    "pprint(test_dl)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5dm7le-6J3l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155417772,
     "user_tz": -120,
     "elapsed": 543,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "4a9be4f4-307a-4ce0-e500-87729ab17d5e",
    "pycharm": {
     "name": "#%% Preparing dataset for evaluation\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "EGKdEYqg7cmd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "feature_column_dict, feature_column_input_dict = generate_feature_columns()\n",
    "# defining the input to be fed into each model\n",
    "inputs = {**feature_column_input_dict[\"numeric\"], **feature_column_input_dict[\"embedding\"]}\n"
   ],
   "metadata": {
    "id": "pFb3xkST6NLl",
    "pycharm": {
     "name": "#%% Creating TF feature columns\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "bm9jx3wy6QD8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "models_dir = './deep_and_wide/NN_checkpoint'\n",
    "if not os.path.isdir(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "# create the folders to save the checkpoints\n",
    "wmodel_dir = models_dir + '/Wide'\n",
    "dmodel_dir = models_dir + '/Deep'\n",
    "wdmodel_dir = models_dir + '/W&D'\n",
    "bayesian_dir = models_dir + '/Bayesian'\n",
    "os.makedirs(wmodel_dir, exist_ok=True)\n",
    "os.makedirs(dmodel_dir, exist_ok=True)\n",
    "os.makedirs(wdmodel_dir, exist_ok=True)\n",
    "os.makedirs(bayesian_dir, exist_ok=True)\n",
    "# setting the hyperparameters\n",
    "lr = 1e-3\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "AKoUaQ1n6TVC",
    "pycharm": {
     "name": "#%% Models\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# WIDE MODEL ONLY"
   ],
   "metadata": {
    "id": "9i2zZ-VZ7r2C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pKOCiH_N7vtx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wmodel, wmodel_path, w_es, w_mc = build_wide_model(feature_column_dict,\n",
    "                                                   inputs,\n",
    "                                                   wmodel_dir=wmodel_dir)\n",
    "wmodel.summary()  # To display the architecture"
   ],
   "metadata": {
    "id": "ovlsD2r06XJB",
    "pycharm": {
     "name": "#%% Wide only model\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "dQYU-JJT6a3i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    w_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=wmodel_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H = wmodel.fit(train_dl,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=n_epochs,\n",
    "                   validation_data=val_dl,\n",
    "                   shuffle=False,\n",
    "                   validation_batch_size=batch_size,\n",
    "                   callbacks=[w_es, w_mc, w_m])\n",
    "else:\n",
    "    wmodel = tf.keras.models.load_model(wmodel_path)"
   ],
   "metadata": {
    "id": "uMiujZJ36eFa",
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "tLPOL4G973TV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eval_wmodel_train = wmodel.evaluate(train_dl)\n",
    "eval_wmodel_val = wmodel.evaluate(val_dl)\n",
    "eval_wmodel_test = wmodel.evaluate(test_dl)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_wmodel_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_wmodel_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_wmodel_test)"
   ],
   "metadata": {
    "id": "24kCeYDY6hIC",
    "pycharm": {
     "name": "#%% Generate the predictions\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# DEEP MODEL ONLY"
   ],
   "metadata": {
    "id": "tZp9xCPa6lAX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## With only embeddings"
   ],
   "metadata": {
    "id": "-pQzT_yY6ogo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dmodel_1_emb, dmodel_1_emb_path, d1_es, d1_mc = build_deep_model(feature_column_dict[\"embedding\"],\n",
    "                                                                 inputs,\n",
    "                                                                 dmodel_dir,\n",
    "                                                                 name=\"dmodel_1_emb.h5\",\n",
    "                                                                 ckpt_name=\"dmodel_1_emb_checkpoint.h5\")\n",
    "dmodel_1_emb.summary()  # To display the architecture"
   ],
   "metadata": {
    "id": "2_FC9BqC798F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "dUaErCdo6rXX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    d1_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dmodel_1_emb_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H1 = dmodel_1_emb.fit(train_dl,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=val_dl,\n",
    "                          shuffle=False,\n",
    "                          validation_batch_size=batch_size,\n",
    "                          callbacks=[d1_es, d1_mc, d1_m])\n",
    "else:\n",
    "    dmodel_1_emb = tf.keras.models.load_model(dmodel_1_emb_path)"
   ],
   "metadata": {
    "id": "w54YwsP66uJx",
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "4dUl_Yg66xdc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eval_dmodel_1_emb_train = dmodel_1_emb.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_dmodel_1_emb_val = dmodel_1_emb.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_dmodel_1_emb_test = dmodel_1_emb.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_dmodel_1_emb_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_dmodel_1_emb_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_dmodel_1_emb_test)"
   ],
   "metadata": {
    "id": "e2G6bCQ-8E1I",
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## With only numerical features"
   ],
   "metadata": {
    "id": "-HolSgJc60Ma",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dmodel_2_num, dmodel_2_num_path, d2_es, d2_mc = build_deep_model(feature_column_dict[\"numeric\"],\n",
    "                                                                 inputs,\n",
    "                                                                 dmodel_dir,\n",
    "                                                                 name=\"dmodel_2_num.h5\",\n",
    "                                                                 ckpt_name=\"dmodel_2_num_checkpoint.h5\")\n",
    "dmodel_2_num.summary()\n"
   ],
   "metadata": {
    "id": "YjTQNsOV63Yo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "GvBkY-jB66eC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    d2_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dmodel_2_num_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H2 = dmodel_2_num.fit(train_dl,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=val_dl,\n",
    "                          shuffle=False,\n",
    "                          validation_batch_size=batch_size,\n",
    "                          callbacks=[d2_es, d2_mc, d2_m])\n",
    "else:\n",
    "    dmodel_2_num = tf.keras.models.load_model(dmodel_2_num_path)"
   ],
   "metadata": {
    "id": "-1q7mWTy8LoU",
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0S5eOfdh69XJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155470121,
     "user_tz": -120,
     "elapsed": 604,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "316e295a-ebf1-4cfe-fc32-c8d834a7c262",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " camp_f0 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " camp_f1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " camp_id (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_f0 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_f1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_id (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " deep_feature_layer (DenseFeatu  (None, 27)          1670        ['camp_f0[0][0]',                \n",
      " res)                                                             'camp_f1[0][0]',                \n",
      "                                                                  'camp_id[0][0]',                \n",
      "                                                                  'user_f0[0][0]',                \n",
      "                                                                  'user_f1[0][0]',                \n",
      "                                                                  'user_id[0][0]']                \n",
      "                                                                                                  \n",
      " deep_fc_1 (Dense)              (None, 512)          14336       ['deep_feature_layer[0][0]']     \n",
      "                                                                                                  \n",
      " deep_fc_2 (Dense)              (None, 256)          131328      ['deep_fc_1[0][0]']              \n",
      "                                                                                                  \n",
      " wide_feature_layer (DenseFeatu  (None, 1000)        0           ['camp_f0[0][0]',                \n",
      " res)                                                             'camp_f1[0][0]',                \n",
      "                                                                  'camp_id[0][0]',                \n",
      "                                                                  'user_f0[0][0]',                \n",
      "                                                                  'user_f1[0][0]',                \n",
      "                                                                  'user_id[0][0]']                \n",
      "                                                                                                  \n",
      " deep_fc_3 (Dense)              (None, 128)          32896       ['deep_fc_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1128)         0           ['wide_feature_layer[0][0]',     \n",
      "                                                                  'deep_fc_3[0][0]']              \n",
      "                                                                                                  \n",
      " deep_output (Dense)            (None, 10)           11290       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 191,520\n",
      "Trainable params: 191,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "eval_dmodel_2_num_train = dmodel_2_num.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_dmodel_2_num_val = dmodel_2_num.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_dmodel_2_num_test = dmodel_2_num.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_dmodel_2_num_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_dmodel_2_num_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_dmodel_2_num_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTPI3ZeN7AXD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155491528,
     "user_tz": -120,
     "elapsed": 11775,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "ee6f0ae8-39b0-4cd7-d49e-24dc0396fd27",
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['cohort', 'user_fh', 'camp_fh'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5/7 [====================>.........] - ETA: 0s - loss: 2.2138 - accuracy: 0.3440 - auc: 0.7955 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.58209, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.82258, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 4s 276ms/step - loss: 2.1809 - accuracy: 0.3727 - auc: 0.8281 - val_loss: 1.8226 - val_accuracy: 0.5821 - val_auc: 0.9567\n",
      "Epoch 2/100\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.5726 - accuracy: 0.5800 - auc: 0.9523\n",
      "Epoch 2: val_accuracy did not improve from 0.58209\n",
      "\n",
      "Epoch 2: val_loss improved from 1.82258 to 0.93429, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 1.5016 - accuracy: 0.5740 - auc: 0.9512 - val_loss: 0.9343 - val_accuracy: 0.5672 - val_auc: 0.9590\n",
      "Epoch 3/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.7772 - accuracy: 0.5717 - auc: 0.9605\n",
      "Epoch 3: val_accuracy improved from 0.58209 to 0.62687, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.93429 to 0.68490, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 0.7771 - accuracy: 0.5724 - auc: 0.9605 - val_loss: 0.6849 - val_accuracy: 0.6269 - val_auc: 0.9618\n",
      "Epoch 4/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6744 - accuracy: 0.6067 - auc: 0.9597\n",
      "Epoch 4: val_accuracy did not improve from 0.62687\n",
      "\n",
      "Epoch 4: val_loss improved from 0.68490 to 0.64837, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.6746 - accuracy: 0.6057 - auc: 0.9596 - val_loss: 0.6484 - val_accuracy: 0.6070 - val_auc: 0.9637\n",
      "Epoch 5/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6337 - accuracy: 0.6317 - auc: 0.9684\n",
      "Epoch 5: val_accuracy improved from 0.62687 to 0.66667, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.64837\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.6338 - accuracy: 0.6323 - auc: 0.9683 - val_loss: 0.6544 - val_accuracy: 0.6667 - val_auc: 0.9637\n",
      "Epoch 6/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6333 - accuracy: 0.6800 - auc: 0.9676\n",
      "Epoch 6: val_accuracy did not improve from 0.66667\n",
      "\n",
      "Epoch 6: val_loss improved from 0.64837 to 0.64797, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6327 - accuracy: 0.6805 - auc: 0.9677 - val_loss: 0.6480 - val_accuracy: 0.5871 - val_auc: 0.9619\n",
      "Epoch 7/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6476 - accuracy: 0.6133 - auc: 0.9650\n",
      "Epoch 7: val_accuracy did not improve from 0.66667\n",
      "\n",
      "Epoch 7: val_loss improved from 0.64797 to 0.62841, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.6471 - accuracy: 0.6140 - auc: 0.9650 - val_loss: 0.6284 - val_accuracy: 0.6517 - val_auc: 0.9668\n",
      "Epoch 8/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5944 - accuracy: 0.6833 - auc: 0.9715\n",
      "Epoch 8: val_accuracy improved from 0.66667 to 0.67662, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.62841 to 0.62632, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 0.5936 - accuracy: 0.6839 - auc: 0.9716 - val_loss: 0.6263 - val_accuracy: 0.6766 - val_auc: 0.9672\n",
      "Epoch 9/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5914 - accuracy: 0.6683 - auc: 0.9710\n",
      "Epoch 9: val_accuracy did not improve from 0.67662\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.62632\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.5913 - accuracy: 0.6689 - auc: 0.9710 - val_loss: 0.6391 - val_accuracy: 0.6517 - val_auc: 0.9660\n",
      "Epoch 10/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6114 - accuracy: 0.6817 - auc: 0.9698\n",
      "Epoch 10: val_accuracy improved from 0.67662 to 0.68657, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.62632 to 0.60847, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 0.6112 - accuracy: 0.6822 - auc: 0.9699 - val_loss: 0.6085 - val_accuracy: 0.6866 - val_auc: 0.9703\n",
      "Epoch 11/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6131 - accuracy: 0.6433 - auc: 0.9705\n",
      "Epoch 11: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.60847\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.6138 - accuracy: 0.6423 - auc: 0.9704 - val_loss: 0.6225 - val_accuracy: 0.6070 - val_auc: 0.9673\n",
      "Epoch 12/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5713 - accuracy: 0.6900 - auc: 0.9739\n",
      "Epoch 12: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.60847\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.5715 - accuracy: 0.6905 - auc: 0.9739 - val_loss: 0.6385 - val_accuracy: 0.6517 - val_auc: 0.9654\n",
      "Epoch 13/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6143 - accuracy: 0.6667 - auc: 0.9694\n",
      "Epoch 13: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.60847\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.6144 - accuracy: 0.6672 - auc: 0.9694 - val_loss: 0.6149 - val_accuracy: 0.6816 - val_auc: 0.9688\n",
      "Epoch 14/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5935 - accuracy: 0.6533 - auc: 0.9712\n",
      "Epoch 14: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.60847\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.5931 - accuracy: 0.6539 - auc: 0.9713 - val_loss: 0.6843 - val_accuracy: 0.5622 - val_auc: 0.9632\n",
      "Epoch 15/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6044 - accuracy: 0.6350 - auc: 0.9705\n",
      "Epoch 15: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 15: val_loss improved from 0.60847 to 0.60675, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.6051 - accuracy: 0.6339 - auc: 0.9704 - val_loss: 0.6067 - val_accuracy: 0.6667 - val_auc: 0.9698\n",
      "Epoch 16/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5270 - accuracy: 0.7283 - auc: 0.9799\n",
      "Epoch 16: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.60675\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.5283 - accuracy: 0.7271 - auc: 0.9797 - val_loss: 0.6177 - val_accuracy: 0.6716 - val_auc: 0.9683\n",
      "Epoch 17/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5664 - accuracy: 0.6983 - auc: 0.9764\n",
      "Epoch 17: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.60675\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.5666 - accuracy: 0.6988 - auc: 0.9763 - val_loss: 0.7017 - val_accuracy: 0.5920 - val_auc: 0.9577\n",
      "Epoch 18/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6055 - accuracy: 0.6750 - auc: 0.9707\n",
      "Epoch 18: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.60675\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.6051 - accuracy: 0.6755 - auc: 0.9707 - val_loss: 0.6234 - val_accuracy: 0.6318 - val_auc: 0.9671\n",
      "Epoch 19/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5319 - accuracy: 0.7300 - auc: 0.9807\n",
      "Epoch 19: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.60675\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.5325 - accuracy: 0.7288 - auc: 0.9806 - val_loss: 0.6100 - val_accuracy: 0.6716 - val_auc: 0.9692\n",
      "Epoch 20/100\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5587 - accuracy: 0.7080 - auc: 0.9755\n",
      "Epoch 20: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.60675\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.5459 - accuracy: 0.7205 - auc: 0.9769 - val_loss: 0.6178 - val_accuracy: 0.6368 - val_auc: 0.9677\n",
      "Epoch 21/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5236 - accuracy: 0.7483 - auc: 0.9800\n",
      "Epoch 21: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 21: val_loss improved from 0.60675 to 0.60183, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.5230 - accuracy: 0.7488 - auc: 0.9801 - val_loss: 0.6018 - val_accuracy: 0.6766 - val_auc: 0.9703\n",
      "Epoch 22/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4995 - accuracy: 0.7550 - auc: 0.9827\n",
      "Epoch 22: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4988 - accuracy: 0.7554 - auc: 0.9827 - val_loss: 0.6090 - val_accuracy: 0.6766 - val_auc: 0.9699\n",
      "Epoch 23/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4893 - accuracy: 0.7383 - auc: 0.9831\n",
      "Epoch 23: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4890 - accuracy: 0.7388 - auc: 0.9832 - val_loss: 0.6142 - val_accuracy: 0.6667 - val_auc: 0.9698\n",
      "Epoch 24/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4816 - accuracy: 0.7350 - auc: 0.9834\n",
      "Epoch 24: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.4825 - accuracy: 0.7338 - auc: 0.9833 - val_loss: 0.6194 - val_accuracy: 0.6716 - val_auc: 0.9696\n",
      "Epoch 25/100\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4636 - accuracy: 0.7760 - auc: 0.9848\n",
      "Epoch 25: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4852 - accuracy: 0.7654 - auc: 0.9832 - val_loss: 0.6218 - val_accuracy: 0.6667 - val_auc: 0.9692\n",
      "Epoch 26/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4588 - accuracy: 0.7867 - auc: 0.9863\n",
      "Epoch 26: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4601 - accuracy: 0.7854 - auc: 0.9862 - val_loss: 0.6779 - val_accuracy: 0.6169 - val_auc: 0.9643\n",
      "Epoch 27/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5696 - accuracy: 0.6867 - auc: 0.9758\n",
      "Epoch 27: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.5689 - accuracy: 0.6872 - auc: 0.9758 - val_loss: 0.7468 - val_accuracy: 0.6020 - val_auc: 0.9587\n",
      "Epoch 28/100\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5166 - accuracy: 0.7160 - auc: 0.9804\n",
      "Epoch 28: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.5043 - accuracy: 0.7354 - auc: 0.9816 - val_loss: 0.6434 - val_accuracy: 0.6468 - val_auc: 0.9663\n",
      "Epoch 29/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4646 - accuracy: 0.7700 - auc: 0.9855\n",
      "Epoch 29: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4641 - accuracy: 0.7704 - auc: 0.9856 - val_loss: 0.6446 - val_accuracy: 0.6716 - val_auc: 0.9680\n",
      "Epoch 30/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4623 - accuracy: 0.7750 - auc: 0.9850\n",
      "Epoch 30: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4623 - accuracy: 0.7754 - auc: 0.9850 - val_loss: 0.6477 - val_accuracy: 0.6418 - val_auc: 0.9681\n",
      "Epoch 31/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4546 - accuracy: 0.7567 - auc: 0.9853\n",
      "Epoch 31: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4540 - accuracy: 0.7571 - auc: 0.9853 - val_loss: 0.6805 - val_accuracy: 0.6368 - val_auc: 0.9670\n",
      "Epoch 32/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4534 - accuracy: 0.7667 - auc: 0.9853\n",
      "Epoch 32: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.4540 - accuracy: 0.7654 - auc: 0.9852 - val_loss: 0.6673 - val_accuracy: 0.6617 - val_auc: 0.9676\n",
      "Epoch 33/100\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.4491 - accuracy: 0.7800 - auc: 0.9857\n",
      "Epoch 33: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4543 - accuracy: 0.7687 - auc: 0.9853 - val_loss: 0.7063 - val_accuracy: 0.6020 - val_auc: 0.9639\n",
      "Epoch 34/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4579 - accuracy: 0.7633 - auc: 0.9853\n",
      "Epoch 34: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4578 - accuracy: 0.7637 - auc: 0.9853 - val_loss: 0.6770 - val_accuracy: 0.5970 - val_auc: 0.9651\n",
      "Epoch 35/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4338 - accuracy: 0.8033 - auc: 0.9872\n",
      "Epoch 35: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4338 - accuracy: 0.8037 - auc: 0.9872 - val_loss: 0.6730 - val_accuracy: 0.6418 - val_auc: 0.9664\n",
      "Epoch 36/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4266 - accuracy: 0.7967 - auc: 0.9875\n",
      "Epoch 36: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4260 - accuracy: 0.7970 - auc: 0.9876 - val_loss: 0.6815 - val_accuracy: 0.6269 - val_auc: 0.9668\n",
      "Epoch 37/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4260 - accuracy: 0.7883 - auc: 0.9873\n",
      "Epoch 37: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.4274 - accuracy: 0.7870 - auc: 0.9872 - val_loss: 0.6741 - val_accuracy: 0.6318 - val_auc: 0.9675\n",
      "Epoch 38/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4319 - accuracy: 0.7950 - auc: 0.9867\n",
      "Epoch 38: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.4323 - accuracy: 0.7953 - auc: 0.9867 - val_loss: 0.7042 - val_accuracy: 0.6269 - val_auc: 0.9658\n",
      "Epoch 39/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4866 - accuracy: 0.7383 - auc: 0.9821\n",
      "Epoch 39: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4861 - accuracy: 0.7388 - auc: 0.9822 - val_loss: 0.6754 - val_accuracy: 0.6119 - val_auc: 0.9655\n",
      "Epoch 40/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4390 - accuracy: 0.7933 - auc: 0.9867\n",
      "Epoch 40: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.4387 - accuracy: 0.7937 - auc: 0.9867 - val_loss: 0.6914 - val_accuracy: 0.6219 - val_auc: 0.9644\n",
      "Epoch 41/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4173 - accuracy: 0.8083 - auc: 0.9885\n",
      "Epoch 41: val_accuracy did not improve from 0.68657\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.60183\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4173 - accuracy: 0.8087 - auc: 0.9885 - val_loss: 0.6812 - val_accuracy: 0.6468 - val_auc: 0.9672\n",
      "Epoch 41: early stopping\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## With embeddings and numerical features"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwB2f8x87DfD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660155498688,
     "user_tz": -120,
     "elapsed": 566,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "ac24fc2a-7292-48a5-c0f1-6dda5991ef25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4051 - accuracy: 0.8153 - auc: 0.9889\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6812 - accuracy: 0.6468 - auc: 0.9672\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5537 - accuracy: 0.7250 - auc: 0.9781\n",
      "\n",
      "[INFO] On Training Set:\n",
      "[0.40510353446006775, 0.8153077960014343, 0.9889481663703918]\n",
      "\n",
      "[INFO] On Validation Set:\n",
      "[0.6811900734901428, 0.646766185760498, 0.9671762585639954]\n",
      "\n",
      "[INFO] On Test Set:\n",
      "[0.5536547303199768, 0.7250000238418579, 0.9780610799789429]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dmodel_3_num_emb, dmodel_3_num_emb_path, d3_es, d3_mc = build_deep_model(feature_column_dict,\n",
    "                                                                         inputs,\n",
    "                                                                         dmodel_dir,\n",
    "                                                                         name=\"dmodel_3_num_emb.h5\",\n",
    "                                                                         ckpt_name=\"dmodel_3_num_emb_checkpoint.h5\")\n",
    "dmodel_3_num_emb.summary()"
   ],
   "metadata": {
    "id": "YZjYyD828Qd8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "iWc9QHoZ7GRI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    d3_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dmodel_3_num_emb_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H3 = dmodel_3_num_emb.fit(train_dl,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=n_epochs,\n",
    "                              validation_data=val_dl,\n",
    "                              shuffle=False,\n",
    "                              validation_batch_size=batch_size,\n",
    "                              callbacks=[d3_es, d3_mc, d3_m])\n",
    "else:\n",
    "    dmodel_3_num_emb = tf.keras.models.load_model(dmodel_3_num_emb_path)"
   ],
   "metadata": {
    "id": "0USCiGEr7I1_",
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ET3UZgYY7Lg9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_dmodel_3_num_emb_train = dmodel_3_num_emb.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_dmodel_3_num_emb_val = dmodel_3_num_emb.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_dmodel_3_num_emb_test = dmodel_3_num_emb.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_dmodel_3_num_emb_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_dmodel_3_num_emb_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_dmodel_3_num_emb_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## With normal and hidden numeric features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the new feature column and input dicts\n",
    "feature_column_dict_hidden, feature_column_input_dict_hidden = generate_feature_columns(hidden_include=True)\n",
    "inputs_hidden = {**feature_column_input_dict_hidden[\"numeric\"], **feature_column_input_dict_hidden[\"embedding\"]}\n",
    "dmodel_4_hid, dmodel_4_hid_path, d4_es, d4_mc = build_deep_model(feature_column_dict_hidden,\n",
    "                                                                 inputs_hidden,\n",
    "                                                                 dmodel_dir,\n",
    "                                                                 name=\"dmodel_4_hid.h5\",\n",
    "                                                                 ckpt_name=\"dmodel_4_hid_checkpoint.h5\")\n",
    "dmodel_4_hid.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    d4_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dmodel_4_hid_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H4 = dmodel_4_hid.fit(train_dl,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=val_dl,\n",
    "                          shuffle=False,\n",
    "                          validation_batch_size=batch_size,\n",
    "                          callbacks=[d4_es, d4_mc, d4_m])\n",
    "else:\n",
    "    dmodel_4_hid = tf.keras.models.load_model(dmodel_4_hid_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_dmodel_4_hid_train = dmodel_4_hid.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_dmodel_4_hid_val = dmodel_4_hid.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_dmodel_4_hid_test = dmodel_4_hid.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_dmodel_4_hid_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_dmodel_4_hid_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_dmodel_4_hid_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wide & deep model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wdmodel, wdmodel_path, wd_es, wd_mc = build_wide_and_deep_model(feature_column_dict,\n",
    "                                                                inputs,\n",
    "                                                                wdmodel_dir=wdmodel_dir)\n",
    "wdmodel.summary()  # To display the architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    wd_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=wdmodel_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H = wdmodel.fit(train_dl,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs,\n",
    "                    validation_data=val_dl,\n",
    "                    shuffle=False,\n",
    "                    validation_batch_size=batch_size,\n",
    "                    callbacks=[wd_es, wd_mc, wd_m])\n",
    "else:\n",
    "    wdmodel = tf.keras.models.load_model(wdmodel_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_wdmodel_train = wdmodel.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_wdmodel_val = wdmodel.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_wdmodel_test = wdmodel.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_wdmodel_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_wdmodel_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_wdmodel_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Bayesian Wide & deep model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bmodel, bmodel_path, b_es, b_mc = build_bayesian_model(feature_column_dict,\n",
    "                                                       inputs,\n",
    "                                                       bayesian_dir)\n",
    "bmodel.summary()  # To display the architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    b_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=bmodel_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H = bmodel.fit(train_dl,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=n_epochs,\n",
    "                   validation_data=val_dl,\n",
    "                   shuffle=False,\n",
    "                   validation_batch_size=batch_size,\n",
    "                   callbacks=[b_es, b_mc, b_m])\n",
    "else:\n",
    "    bmodel = tf.keras.models.load_model(bmodel_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts_train, ucb_train = evaluate_bandit(bmodel, train_dl)\n",
    "ts_val, ucb_val = evaluate_bandit(bmodel, val_dl)\n",
    "ts_test, ucb_test = evaluate_bandit(bmodel, test_dl)\n",
    "# Print the results\n",
    "print(\"\\nUCB\\n[INFO] On Training Set:\")\n",
    "print(ucb_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(ucb_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(ucb_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nThompson Sampling\\n[INFO] On Training Set:\")\n",
    "print(ts_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(ts_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(ts_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    }
   }
  }
 ]
}