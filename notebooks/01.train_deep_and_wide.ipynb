{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "01.train_deep_and_wide.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SETUP the Colab Env."
   ],
   "metadata": {
    "id": "fdHWR9Ay9nEV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # import drive from google colab\n",
    "\n",
    "ROOT = \"/content/drive\"     # default location for the drive\n",
    "print(ROOT)                 # print content of ROOT (Optional)\n",
    "\n",
    "drive.mount(ROOT)           # we mount the google drive at /content/drive"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P8nBjpS87OB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226837189,
     "user_tz": -120,
     "elapsed": 2158,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "5676040a-0a41-4552-e19b-9ff9af158e78",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Clone github repository setup\n",
    "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
    "from os.path import join  \n",
    "\n",
    "# path to your project on Google Drive\n",
    "MY_GOOGLE_DRIVE_PATH = 'MyDrive/Colab Notebooks/'  \n",
    "# Replace with your github repository\n",
    "GIT_REPOSITORY = \"promo-recommendation\" \n",
    "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
    "\n",
    "# It's good to print out the value if you are not sure \n",
    "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
    "\n",
    "# In case we haven't created the folder already; we will create a folder in the project path \n",
    "# !mkdir \"{PROJECT_PATH}\"    \n",
    "\n",
    "GIT_PATH = \"https://\" + \"@github.com/\" + \"fellowship/\" + GIT_REPOSITORY + \".git\"\n",
    "print(\"GIT_PATH: \", GIT_PATH)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ka7vN7zu9Mza",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226839383,
     "user_tz": -120,
     "elapsed": 526,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "8e72553e-09ca-42cd-a9e9-cb5e153f3069",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PROJECT_PATH:  /content/drive/MyDrive/Colab Notebooks/\n",
      "GIT_PATH:  https://@github.com/fellowship/promo-recommendation.git\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd \"{PROJECT_PATH}{\"promo-recommendation/\"}\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Yxv8N149P_l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226843648,
     "user_tz": -120,
     "elapsed": 429,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "b0bbbae7-bcca-4a12-9252-09cf97ecc44c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/promo-recommendation\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network Training"
   ],
   "metadata": {
    "id": "AFjfUsso2Kc5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python ./00.generate_data.py"
   ],
   "metadata": {
    "id": "VcuUGD7v9hLY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rtcr1aOp5WO9",
    "pycharm": {
     "name": "#%%\n"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226894901,
     "user_tz": -120,
     "elapsed": 3211,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import importlib\n",
    "import os\n",
    "from routine.utilities import generate_CSV, df_to_dataloader, generate_feature_columns\n",
    "from routine.data_generation import generate_data\n",
    "from routine.models import build_wide_model, build_deep_model, build_wide_and_deep_model, \\\n",
    "    build_bayesian_model, evaluate_bandit\n",
    "from os.path import exists\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_regenerate = False\n",
    "if data_regenerate:\n",
    "    obs_df, user_df, camp_df = generate_data(\n",
    "        num_users=1000,\n",
    "        num_campaigns=100,\n",
    "        samples_per_campaign=10000,\n",
    "        num_cohort=10,\n",
    "        cohort_variances=np.linspace(0.05, 0.6, 10),\n",
    "        fh_cohort=True,\n",
    "        response_sig_a=10,\n",
    "        even_cohort=True,\n",
    "        cross_response=False,\n",
    "        magnify_hf=1\n",
    "    )\n",
    "else:\n",
    "    obs_df = pd.read_csv('observation_even.csv')\n",
    "\n",
    "\n",
    "INPUT_DATA_PATH = './deep_and_wide/NN_Inputs/input_data'\n",
    "if not os.path.isdir(INPUT_DATA_PATH):\n",
    "    os.makedirs(INPUT_DATA_PATH)"
   ],
   "metadata": {
    "id": "M_5XXlBi50Wb",
    "pycharm": {
     "name": "#%%\n"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226901269,
     "user_tz": -120,
     "elapsed": 2036,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Creating the training, validation, and testing data for the model\n",
    "train_path = INPUT_DATA_PATH + \"/train.csv\"\n",
    "val_path = INPUT_DATA_PATH + \"/val.csv\"\n",
    "test_path = INPUT_DATA_PATH + \"/test.csv\"\n",
    "re_create = False\n",
    "if re_create:\n",
    "    generate_CSV(obs_df,\n",
    "                 train_path,\n",
    "                 val_path,\n",
    "                 test_path,\n",
    "                 verbose=True)"
   ],
   "metadata": {
    "id": "IM2zBp9w6CFI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226905877,
     "user_tz": -120,
     "elapsed": 417,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "pycharm": {
     "name": "#%% Creating the training, validation and testing data for the model\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "obs_df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "qhRc8BkEF4Bq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226908583,
     "user_tz": -120,
     "elapsed": 438,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "af5c8b82-ffe9-4a98-9963-0f21e14d7c7a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0  user_id  camp_id  cohort  user_f0  user_f1  user_fh  camp_f0  \\\n",
       "0           0      917       30       9   -0.252   -0.257    0.339    0.753   \n",
       "1           1      129       59       1    0.088   -0.093   -0.237   -0.484   \n",
       "2           2       60       84       0   -0.179   -0.339   -0.267    0.126   \n",
       "3           3      494       12       4   -0.207    0.210   -0.402   -0.755   \n",
       "4           4      152       97       1    0.079   -0.129   -0.140    0.999   \n",
       "\n",
       "   camp_f1  camp_fh  response  \n",
       "0   -0.555   -0.953         0  \n",
       "1   -0.785   -0.749         1  \n",
       "2    0.440   -0.698         0  \n",
       "3    0.506   -0.926         1  \n",
       "4    0.713   -0.221         1  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-e17aae54-3c37-4342-8d57-e1953ac633a8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>camp_id</th>\n",
       "      <th>cohort</th>\n",
       "      <th>user_f0</th>\n",
       "      <th>user_f1</th>\n",
       "      <th>user_fh</th>\n",
       "      <th>camp_f0</th>\n",
       "      <th>camp_f1</th>\n",
       "      <th>camp_fh</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>917</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.753</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>494</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.713</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e17aae54-3c37-4342-8d57-e1953ac633a8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e17aae54-3c37-4342-8d57-e1953ac633a8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e17aae54-3c37-4342-8d57-e1953ac633a8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Preparing dataset for evaluation\n",
    "batch_size = 500\n",
    "n_epochs = 300\n",
    "feature_columns = [\"user_id\", \"camp_id\", \"cohort\",\n",
    "                   \"user_f0\", \"user_f1\", \"user_fh\",\n",
    "                   \"camp_f0\", \"camp_f1\", \"camp_fh\"]\n",
    "\n",
    "target_column = \"response\"\n",
    "\n",
    "train_dl = df_to_dataloader(train_path,\n",
    "                            feature_columns,\n",
    "                            target_column,\n",
    "                            batch_size=batch_size)\n",
    "val_dl = df_to_dataloader(val_path,\n",
    "                          feature_columns,\n",
    "                          target_column,\n",
    "                          batch_size=batch_size)\n",
    "test_dl = df_to_dataloader(test_path,\n",
    "                           feature_columns,\n",
    "                           target_column,\n",
    "                           shuffle=False,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "print(\"[INFO] Train dataloader:\")\n",
    "pprint(train_dl)\n",
    "print(\"[INFO] Val dataloader:\")\n",
    "pprint(val_dl)\n",
    "print(\"[INFO] Test dataloader:\")\n",
    "pprint(test_dl)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5dm7le-6J3l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226923607,
     "user_tz": -120,
     "elapsed": 3128,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "18ae6589-40bc-49c5-9296-4676c965bf5d",
    "pycharm": {
     "name": "#%% Preparing dataset for evaluation\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] Train dataloader:\n",
      "<BatchDataset element_spec=({'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'camp_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'cohort': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None)}, TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>\n",
      "[INFO] Val dataloader:\n",
      "<BatchDataset element_spec=({'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'camp_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'cohort': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None)}, TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>\n",
      "[INFO] Test dataloader:\n",
      "<BatchDataset element_spec=({'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'camp_id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'cohort': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'user_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'user_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f0': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_f1': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'camp_fh': TensorSpec(shape=(None,), dtype=tf.float64, name=None)}, TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Creating TF feature columns\n",
    "feature_column_dict, feature_column_input_dict = generate_feature_columns()\n",
    "# defining the input to be fed into each model\n",
    "inputs = {**feature_column_input_dict[\"numeric\"], **feature_column_input_dict[\"embedding\"]}\n"
   ],
   "metadata": {
    "id": "pFb3xkST6NLl",
    "pycharm": {
     "name": "#%% Creating TF feature columns\n"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226959081,
     "user_tz": -120,
     "elapsed": 417,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(feature_column_dict)\n",
    "print(feature_column_input_dict)\n",
    "print(inputs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOzyw4eOPIXN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660226962646,
     "user_tz": -120,
     "elapsed": 430,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "124192bd-15c6-48a3-f528-457e745a12df",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'numeric': [NumericColumn(key='user_f0', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='user_f1', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='camp_f0', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='camp_f1', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)], 'embedding': [EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='user_id', hash_bucket_size=1000, dtype=tf.int64), dimension=16, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7fe5000bb950>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='camp_id', hash_bucket_size=100, dtype=tf.int64), dimension=7, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7fe5000f6ed0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cohort', hash_bucket_size=10, dtype=tf.int64), dimension=7, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7fe5000b8d90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)], 'crossed': [IndicatorColumn(categorical_column=CrossedColumn(keys=('user_id', 'camp_id'), hash_bucket_size=100000, hash_key=None))]}\n",
      "{'numeric': {'user_f0': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'user_f0')>, 'user_f1': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'user_f1')>, 'camp_f0': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'camp_f0')>, 'camp_f1': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'camp_f1')>}, 'embedding': {'user_id': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'user_id')>, 'camp_id': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'camp_id')>, 'cohort': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'cohort')>}}\n",
      "{'user_f0': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'user_f0')>, 'user_f1': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'user_f1')>, 'camp_f0': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'camp_f0')>, 'camp_f1': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'camp_f1')>, 'user_id': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'user_id')>, 'camp_id': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'camp_id')>, 'cohort': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'cohort')>}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Models\n",
    "models_dir = './deep_and_wide/NN_checkpoint'\n",
    "if not os.path.isdir(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "# create the folders to save the checkpoints\n",
    "wmodel_dir = models_dir + '/Wide'\n",
    "dmodel_dir = models_dir + '/Deep'\n",
    "wdmodel_dir = models_dir + '/W&D'\n",
    "bayesian_dir = models_dir + '/Bayesian'\n",
    "os.makedirs(wmodel_dir, exist_ok=True)\n",
    "os.makedirs(dmodel_dir, exist_ok=True)\n",
    "os.makedirs(wdmodel_dir, exist_ok=True)\n",
    "os.makedirs(bayesian_dir, exist_ok=True)\n",
    "# setting the hyperparameters\n",
    "lr = 1e-3\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "AKoUaQ1n6TVC",
    "pycharm": {
     "name": "#%% Models\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660227129638,
     "user_tz": -120,
     "elapsed": 433,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "178e59ff-66ea-426d-dee4-ef9aa64cb33d"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WIDE MODEL ONLY"
   ],
   "metadata": {
    "id": "9i2zZ-VZ7r2C",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "wmodel, wmodel_path, w_es, w_mc = build_wide_model(feature_column_dict,\n",
    "                                                   inputs,\n",
    "                                                   wmodel_dir=wmodel_dir)\n",
    "wmodel.summary()  # To display the architecture"
   ],
   "metadata": {
    "id": "ovlsD2r06XJB",
    "pycharm": {
     "name": "#%% Wide only model\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    w_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=wmodel_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H = wmodel.fit(train_dl,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=n_epochs,\n",
    "                   validation_data=val_dl,\n",
    "                   shuffle=False,\n",
    "                   validation_batch_size=batch_size,\n",
    "                   callbacks=[w_es, w_mc, w_m])\n",
    "else:\n",
    "    wmodel = tf.keras.models.load_model(wmodel_path)"
   ],
   "metadata": {
    "id": "uMiujZJ36eFa",
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eval_wmodel_train = wmodel.evaluate(train_dl)\n",
    "eval_wmodel_val = wmodel.evaluate(val_dl)\n",
    "eval_wmodel_test = wmodel.evaluate(test_dl)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_wmodel_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_wmodel_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_wmodel_test)"
   ],
   "metadata": {
    "id": "24kCeYDY6hIC",
    "pycharm": {
     "name": "#%% Generate the predictions\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DEEP MODEL ONLY"
   ],
   "metadata": {
    "id": "tZp9xCPa6lAX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. With only embeddings"
   ],
   "metadata": {
    "id": "-pQzT_yY6ogo",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dmodel_1_emb, dmodel_1_emb_path, d1_es, d1_mc = build_deep_model(feature_column_dict[\"embedding\"],\n",
    "                                                                 inputs,\n",
    "                                                                 dmodel_dir,\n",
    "                                                                 name=\"dmodel_1_emb.h5\",\n",
    "                                                                 ckpt_name=\"dmodel_1_emb_checkpoint.h5\")\n",
    "dmodel_1_emb.summary()  # To display the architecture"
   ],
   "metadata": {
    "id": "2_FC9BqC798F",
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660228398997,
     "user_tz": -120,
     "elapsed": 1088,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "344db70c-fd13-4511-c7e7-4a1c2461de47"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " camp_f0 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " camp_f1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " camp_id (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " cohort (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_f0 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_f1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_id (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " dense_feature_layer (DenseFeat  (None, 30)          16770       ['camp_f0[0][0]',                \n",
      " ures)                                                            'camp_f1[0][0]',                \n",
      "                                                                  'camp_id[0][0]',                \n",
      "                                                                  'cohort[0][0]',                 \n",
      "                                                                  'user_f0[0][0]',                \n",
      "                                                                  'user_f1[0][0]',                \n",
      "                                                                  'user_id[0][0]']                \n",
      "                                                                                                  \n",
      " fc_1 (Dense)                   (None, 512)          15872       ['dense_feature_layer[0][0]']    \n",
      "                                                                                                  \n",
      " fc_2 (Dense)                   (None, 256)          131328      ['fc_1[0][0]']                   \n",
      "                                                                                                  \n",
      " fc_3 (Dense)                   (None, 128)          32896       ['fc_2[0][0]']                   \n",
      "                                                                                                  \n",
      " deep_output (Dense)            (None, 10)           1290        ['fc_3[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 198,156\n",
      "Trainable params: 198,156\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    d1_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dmodel_1_emb_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H1 = dmodel_1_emb.fit(train_dl,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=val_dl,\n",
    "                          shuffle=False,\n",
    "                          validation_batch_size=batch_size,\n",
    "                          callbacks=[d1_es, d1_mc, d1_m])\n",
    "else:\n",
    "    dmodel_1_emb = tf.keras.models.load_model(dmodel_1_emb_path)"
   ],
   "metadata": {
    "id": "w54YwsP66uJx",
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660229033997,
     "user_tz": -120,
     "elapsed": 624425,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "122383a8-290d-4615-9d48-96abf80353db"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['user_fh', 'camp_fh'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.5973 - accuracy: 0.6748 - auc_1: 0.9719\n",
      "Epoch 1: val_accuracy improved from -inf to 0.71434, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb_checkpoint.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.54790, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb.h5\n",
      "1201/1201 [==============================] - 20s 14ms/step - loss: 0.5972 - accuracy: 0.6749 - auc_1: 0.9719 - val_loss: 0.5479 - val_accuracy: 0.7143 - val_auc_1: 0.9772\n",
      "Epoch 2/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.5084 - accuracy: 0.7408 - auc_1: 0.9806\n",
      "Epoch 2: val_accuracy improved from 0.71434 to 0.74348, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb_checkpoint.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.54790 to 0.50504, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb.h5\n",
      "1201/1201 [==============================] - 20s 15ms/step - loss: 0.5084 - accuracy: 0.7408 - auc_1: 0.9806 - val_loss: 0.5050 - val_accuracy: 0.7435 - val_auc_1: 0.9809\n",
      "Epoch 3/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.4945 - accuracy: 0.7481 - auc_1: 0.9817\n",
      "Epoch 3: val_accuracy improved from 0.74348 to 0.74473, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb_checkpoint.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.50504 to 0.49763, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb.h5\n",
      "1201/1201 [==============================] - 20s 15ms/step - loss: 0.4945 - accuracy: 0.7481 - auc_1: 0.9817 - val_loss: 0.4976 - val_accuracy: 0.7447 - val_auc_1: 0.9814\n",
      "Epoch 4/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4891 - accuracy: 0.7510 - auc_1: 0.9821\n",
      "Epoch 4: val_accuracy improved from 0.74473 to 0.74742, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb_checkpoint.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.49763 to 0.49582, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb.h5\n",
      "1201/1201 [==============================] - 20s 15ms/step - loss: 0.4890 - accuracy: 0.7511 - auc_1: 0.9821 - val_loss: 0.4958 - val_accuracy: 0.7474 - val_auc_1: 0.9816\n",
      "Epoch 5/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4862 - accuracy: 0.7527 - auc_1: 0.9823\n",
      "Epoch 5: val_accuracy improved from 0.74742 to 0.74983, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb_checkpoint.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.49582 to 0.49203, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb.h5\n",
      "1201/1201 [==============================] - 20s 15ms/step - loss: 0.4862 - accuracy: 0.7527 - auc_1: 0.9823 - val_loss: 0.4920 - val_accuracy: 0.7498 - val_auc_1: 0.9819\n",
      "Epoch 6/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.7536 - auc_1: 0.9825\n",
      "Epoch 6: val_accuracy did not improve from 0.74983\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.49203\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4840 - accuracy: 0.7536 - auc_1: 0.9825 - val_loss: 0.4922 - val_accuracy: 0.7489 - val_auc_1: 0.9818\n",
      "Epoch 7/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.7549 - auc_1: 0.9826\n",
      "Epoch 7: val_accuracy did not improve from 0.74983\n",
      "\n",
      "Epoch 7: val_loss improved from 0.49203 to 0.49059, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb.h5\n",
      "1201/1201 [==============================] - 19s 14ms/step - loss: 0.4823 - accuracy: 0.7549 - auc_1: 0.9826 - val_loss: 0.4906 - val_accuracy: 0.7498 - val_auc_1: 0.9819\n",
      "Epoch 8/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.4804 - accuracy: 0.7554 - auc_1: 0.9827\n",
      "Epoch 8: val_accuracy improved from 0.74983 to 0.75052, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb_checkpoint.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.49059 to 0.48900, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb.h5\n",
      "1201/1201 [==============================] - 20s 15ms/step - loss: 0.4804 - accuracy: 0.7554 - auc_1: 0.9827 - val_loss: 0.4890 - val_accuracy: 0.7505 - val_auc_1: 0.9821\n",
      "Epoch 9/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.7563 - auc_1: 0.9829\n",
      "Epoch 9: val_accuracy did not improve from 0.75052\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.48900\n",
      "1201/1201 [==============================] - 19s 14ms/step - loss: 0.4787 - accuracy: 0.7563 - auc_1: 0.9829 - val_loss: 0.4902 - val_accuracy: 0.7505 - val_auc_1: 0.9820\n",
      "Epoch 10/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.4777 - accuracy: 0.7567 - auc_1: 0.9829\n",
      "Epoch 10: val_accuracy did not improve from 0.75052\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.48900\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4777 - accuracy: 0.7567 - auc_1: 0.9829 - val_loss: 0.4901 - val_accuracy: 0.7498 - val_auc_1: 0.9819\n",
      "Epoch 11/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4771 - accuracy: 0.7573 - auc_1: 0.9830\n",
      "Epoch 11: val_accuracy did not improve from 0.75052\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.48900\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4771 - accuracy: 0.7573 - auc_1: 0.9830 - val_loss: 0.4904 - val_accuracy: 0.7497 - val_auc_1: 0.9819\n",
      "Epoch 12/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7571 - auc_1: 0.9830\n",
      "Epoch 12: val_accuracy improved from 0.75052 to 0.75069, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb_checkpoint.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.48900 to 0.48896, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb.h5\n",
      "1201/1201 [==============================] - 20s 15ms/step - loss: 0.4765 - accuracy: 0.7571 - auc_1: 0.9830 - val_loss: 0.4890 - val_accuracy: 0.7507 - val_auc_1: 0.9821\n",
      "Epoch 13/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.4750 - accuracy: 0.7582 - auc_1: 0.9831\n",
      "Epoch 13: val_accuracy did not improve from 0.75069\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4750 - accuracy: 0.7582 - auc_1: 0.9831 - val_loss: 0.4900 - val_accuracy: 0.7503 - val_auc_1: 0.9820\n",
      "Epoch 14/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.4736 - accuracy: 0.7590 - auc_1: 0.9832\n",
      "Epoch 14: val_accuracy did not improve from 0.75069\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4736 - accuracy: 0.7590 - auc_1: 0.9832 - val_loss: 0.4903 - val_accuracy: 0.7497 - val_auc_1: 0.9819\n",
      "Epoch 15/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.4732 - accuracy: 0.7589 - auc_1: 0.9832\n",
      "Epoch 15: val_accuracy did not improve from 0.75069\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4732 - accuracy: 0.7589 - auc_1: 0.9832 - val_loss: 0.4908 - val_accuracy: 0.7499 - val_auc_1: 0.9819\n",
      "Epoch 16/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4721 - accuracy: 0.7595 - auc_1: 0.9833\n",
      "Epoch 16: val_accuracy did not improve from 0.75069\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4721 - accuracy: 0.7595 - auc_1: 0.9833 - val_loss: 0.4909 - val_accuracy: 0.7505 - val_auc_1: 0.9819\n",
      "Epoch 17/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.4714 - accuracy: 0.7597 - auc_1: 0.9833\n",
      "Epoch 17: val_accuracy did not improve from 0.75069\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4714 - accuracy: 0.7597 - auc_1: 0.9833 - val_loss: 0.4908 - val_accuracy: 0.7507 - val_auc_1: 0.9819\n",
      "Epoch 18/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4706 - accuracy: 0.7605 - auc_1: 0.9834\n",
      "Epoch 18: val_accuracy improved from 0.75069 to 0.75100, saving model to ./deep_and_wide/NN_checkpoint/Deep/dmodel_1_emb_checkpoint.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 19s 15ms/step - loss: 0.4706 - accuracy: 0.7605 - auc_1: 0.9834 - val_loss: 0.4902 - val_accuracy: 0.7510 - val_auc_1: 0.9819\n",
      "Epoch 19/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.4701 - accuracy: 0.7604 - auc_1: 0.9834\n",
      "Epoch 19: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4701 - accuracy: 0.7604 - auc_1: 0.9834 - val_loss: 0.4925 - val_accuracy: 0.7498 - val_auc_1: 0.9818\n",
      "Epoch 20/300\n",
      "1197/1201 [============================>.] - ETA: 0s - loss: 0.4694 - accuracy: 0.7608 - auc_1: 0.9835\n",
      "Epoch 20: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4694 - accuracy: 0.7609 - auc_1: 0.9835 - val_loss: 0.4926 - val_accuracy: 0.7499 - val_auc_1: 0.9818\n",
      "Epoch 21/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.4685 - accuracy: 0.7609 - auc_1: 0.9835\n",
      "Epoch 21: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4685 - accuracy: 0.7609 - auc_1: 0.9835 - val_loss: 0.4922 - val_accuracy: 0.7500 - val_auc_1: 0.9818\n",
      "Epoch 22/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.4684 - accuracy: 0.7614 - auc_1: 0.9835\n",
      "Epoch 22: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4684 - accuracy: 0.7614 - auc_1: 0.9835 - val_loss: 0.4944 - val_accuracy: 0.7493 - val_auc_1: 0.9816\n",
      "Epoch 23/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4675 - accuracy: 0.7612 - auc_1: 0.9836\n",
      "Epoch 23: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4675 - accuracy: 0.7612 - auc_1: 0.9836 - val_loss: 0.4944 - val_accuracy: 0.7491 - val_auc_1: 0.9816\n",
      "Epoch 24/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4668 - accuracy: 0.7620 - auc_1: 0.9836\n",
      "Epoch 24: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4668 - accuracy: 0.7620 - auc_1: 0.9836 - val_loss: 0.4945 - val_accuracy: 0.7493 - val_auc_1: 0.9816\n",
      "Epoch 25/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.4662 - accuracy: 0.7623 - auc_1: 0.9837\n",
      "Epoch 25: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 19s 14ms/step - loss: 0.4662 - accuracy: 0.7624 - auc_1: 0.9837 - val_loss: 0.4962 - val_accuracy: 0.7483 - val_auc_1: 0.9814\n",
      "Epoch 26/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.4660 - accuracy: 0.7623 - auc_1: 0.9837\n",
      "Epoch 26: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4661 - accuracy: 0.7623 - auc_1: 0.9837 - val_loss: 0.4954 - val_accuracy: 0.7489 - val_auc_1: 0.9815\n",
      "Epoch 27/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4652 - accuracy: 0.7625 - auc_1: 0.9837\n",
      "Epoch 27: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4652 - accuracy: 0.7625 - auc_1: 0.9837 - val_loss: 0.4944 - val_accuracy: 0.7488 - val_auc_1: 0.9816\n",
      "Epoch 28/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.4646 - accuracy: 0.7631 - auc_1: 0.9838\n",
      "Epoch 28: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4646 - accuracy: 0.7631 - auc_1: 0.9838 - val_loss: 0.4965 - val_accuracy: 0.7487 - val_auc_1: 0.9814\n",
      "Epoch 29/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4642 - accuracy: 0.7636 - auc_1: 0.9838\n",
      "Epoch 29: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4642 - accuracy: 0.7636 - auc_1: 0.9838 - val_loss: 0.4961 - val_accuracy: 0.7489 - val_auc_1: 0.9814\n",
      "Epoch 30/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.7630 - auc_1: 0.9838\n",
      "Epoch 30: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4635 - accuracy: 0.7630 - auc_1: 0.9838 - val_loss: 0.4983 - val_accuracy: 0.7479 - val_auc_1: 0.9812\n",
      "Epoch 31/300\n",
      "1198/1201 [============================>.] - ETA: 0s - loss: 0.4634 - accuracy: 0.7635 - auc_1: 0.9838\n",
      "Epoch 31: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4634 - accuracy: 0.7635 - auc_1: 0.9838 - val_loss: 0.4974 - val_accuracy: 0.7487 - val_auc_1: 0.9813\n",
      "Epoch 32/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.4633 - accuracy: 0.7633 - auc_1: 0.9838\n",
      "Epoch 32: val_accuracy did not improve from 0.75100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.48896\n",
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.4633 - accuracy: 0.7633 - auc_1: 0.9838 - val_loss: 0.4998 - val_accuracy: 0.7480 - val_auc_1: 0.9811\n",
      "Epoch 32: early stopping\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "eval_dmodel_1_emb_train = dmodel_1_emb.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_dmodel_1_emb_val = dmodel_1_emb.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_dmodel_1_emb_test = dmodel_1_emb.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_dmodel_1_emb_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_dmodel_1_emb_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_dmodel_1_emb_test)"
   ],
   "metadata": {
    "id": "e2G6bCQ-8E1I",
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660229117976,
     "user_tz": -120,
     "elapsed": 57068,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "839d4c56-b0a0-48aa-c2d4-dcff6ba451be"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1201/1201 [==============================] - 12s 9ms/step - loss: 0.4572 - accuracy: 0.7663 - auc_1: 0.9843\n",
      "401/401 [==============================] - 4s 9ms/step - loss: 0.4998 - accuracy: 0.7480 - auc_1: 0.9811\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 0.5014 - accuracy: 0.7478 - auc_1: 0.9810\n",
      "\n",
      "[INFO] On Training Set:\n",
      "[0.4571966230869293, 0.7663237452507019, 0.9842695593833923]\n",
      "\n",
      "[INFO] On Validation Set:\n",
      "[0.49976927042007446, 0.7479562759399414, 0.9810992479324341]\n",
      "\n",
      "[INFO] On Test Set:\n",
      "[0.5014410614967346, 0.7478200197219849, 0.9810281991958618]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. With only numerical features"
   ],
   "metadata": {
    "id": "-HolSgJc60Ma",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dmodel_2_num, dmodel_2_num_path, d2_es, d2_mc = build_deep_model(feature_column_dict[\"numeric\"],\n",
    "                                                                 inputs,\n",
    "                                                                 dmodel_dir,\n",
    "                                                                 name=\"dmodel_2_num.h5\",\n",
    "                                                                 ckpt_name=\"dmodel_2_num_checkpoint.h5\")\n",
    "dmodel_2_num.summary()\n"
   ],
   "metadata": {
    "id": "YjTQNsOV63Yo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    d2_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dmodel_2_num_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H2 = dmodel_2_num.fit(train_dl,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=val_dl,\n",
    "                          shuffle=False,\n",
    "                          validation_batch_size=batch_size,\n",
    "                          callbacks=[d2_es, d2_mc, d2_m])\n",
    "else:\n",
    "    dmodel_2_num = tf.keras.models.load_model(dmodel_2_num_path)"
   ],
   "metadata": {
    "id": "-1q7mWTy8LoU",
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eval_dmodel_2_num_train = dmodel_2_num.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_dmodel_2_num_val = dmodel_2_num.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_dmodel_2_num_test = dmodel_2_num.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_dmodel_2_num_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_dmodel_2_num_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_dmodel_2_num_test)"
   ],
   "metadata": {
    "id": "KTPI3ZeN7AXD",
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. With embeddings and numerical features"
   ],
   "metadata": {
    "id": "zwB2f8x87DfD",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dmodel_3_num_emb, dmodel_3_num_emb_path, d3_es, d3_mc = build_deep_model(feature_column_dict,\n",
    "                                                                         inputs,\n",
    "                                                                         dmodel_dir,\n",
    "                                                                         name=\"dmodel_3_num_emb.h5\",\n",
    "                                                                         ckpt_name=\"dmodel_3_num_emb_checkpoint.h5\")\n",
    "dmodel_3_num_emb.summary()"
   ],
   "metadata": {
    "id": "YZjYyD828Qd8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    d3_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dmodel_3_num_emb_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H3 = dmodel_3_num_emb.fit(train_dl,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=n_epochs,\n",
    "                              validation_data=val_dl,\n",
    "                              shuffle=False,\n",
    "                              validation_batch_size=batch_size,\n",
    "                              callbacks=[d3_es, d3_mc, d3_m])\n",
    "else:\n",
    "    dmodel_3_num_emb = tf.keras.models.load_model(dmodel_3_num_emb_path)"
   ],
   "metadata": {
    "id": "0USCiGEr7I1_",
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_dmodel_3_num_emb_train = dmodel_3_num_emb.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_dmodel_3_num_emb_val = dmodel_3_num_emb.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_dmodel_3_num_emb_test = dmodel_3_num_emb.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_dmodel_3_num_emb_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_dmodel_3_num_emb_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_dmodel_3_num_emb_test)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    },
    "id": "sztaRBmRvjUS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. With normal and hidden numeric features"
   ],
   "metadata": {
    "id": "Ut8vj1hSvjUS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the new feature column and input dicts\n",
    "feature_column_dict_hidden, feature_column_input_dict_hidden = generate_feature_columns(hidden_include=True)\n",
    "inputs_hidden = {**feature_column_input_dict_hidden[\"numeric\"], **feature_column_input_dict_hidden[\"embedding\"]}\n",
    "dmodel_4_hid, dmodel_4_hid_path, d4_es, d4_mc = build_deep_model(feature_column_dict_hidden,\n",
    "                                                                 inputs_hidden,\n",
    "                                                                 dmodel_dir,\n",
    "                                                                 name=\"dmodel_4_hid.h5\",\n",
    "                                                                 ckpt_name=\"dmodel_4_hid_checkpoint.h5\")\n",
    "dmodel_4_hid.summary()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "OLUI1Mb7vjUT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    d4_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dmodel_4_hid_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H4 = dmodel_4_hid.fit(train_dl,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=val_dl,\n",
    "                          shuffle=False,\n",
    "                          validation_batch_size=batch_size,\n",
    "                          callbacks=[d4_es, d4_mc, d4_m])\n",
    "else:\n",
    "    dmodel_4_hid = tf.keras.models.load_model(dmodel_4_hid_path)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    },
    "id": "gmhAJrBZvjUU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_dmodel_4_hid_train = dmodel_4_hid.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_dmodel_4_hid_val = dmodel_4_hid.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_dmodel_4_hid_test = dmodel_4_hid.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_dmodel_4_hid_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_dmodel_4_hid_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_dmodel_4_hid_test)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    },
    "id": "06Kn_mYqvjUV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WIDE & DEEP MODEL"
   ],
   "metadata": {
    "id": "8NXwe40dvjUV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " camp_f0 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " camp_f1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " camp_id (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " cohort (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_f0 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_f1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " user_id (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " deep_feature_layer (DenseFeatu  (None, 34)          16770       ['camp_f0[0][0]',                \n",
      " res)                                                             'camp_f1[0][0]',                \n",
      "                                                                  'camp_id[0][0]',                \n",
      "                                                                  'cohort[0][0]',                 \n",
      "                                                                  'user_f0[0][0]',                \n",
      "                                                                  'user_f1[0][0]',                \n",
      "                                                                  'user_id[0][0]']                \n",
      "                                                                                                  \n",
      " deep_fc_1 (Dense)              (None, 512)          17920       ['deep_feature_layer[0][0]']     \n",
      "                                                                                                  \n",
      " deep_fc_2 (Dense)              (None, 256)          131328      ['deep_fc_1[0][0]']              \n",
      "                                                                                                  \n",
      " wide_feature_layer (DenseFeatu  (None, 100000)      0           ['camp_f0[0][0]',                \n",
      " res)                                                             'camp_f1[0][0]',                \n",
      "                                                                  'camp_id[0][0]',                \n",
      "                                                                  'cohort[0][0]',                 \n",
      "                                                                  'user_f0[0][0]',                \n",
      "                                                                  'user_f1[0][0]',                \n",
      "                                                                  'user_id[0][0]']                \n",
      "                                                                                                  \n",
      " deep_fc_3 (Dense)              (None, 128)          32896       ['deep_fc_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 100128)       0           ['wide_feature_layer[0][0]',     \n",
      "                                                                  'deep_fc_3[0][0]']              \n",
      "                                                                                                  \n",
      " deep_output (Dense)            (None, 10)           1001290     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,200,204\n",
      "Trainable params: 1,200,204\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wdmodel, wdmodel_path, wd_es, wd_mc = build_wide_and_deep_model(feature_column_dict,\n",
    "                                                                inputs,\n",
    "                                                                wdmodel_dir=wdmodel_dir)\n",
    "wdmodel.summary()  # To display the architecture"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dZQjELI2vjUV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660227142405,
     "user_tz": -120,
     "elapsed": 987,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "aad76270-97fa-4388-a912-7b5fce7ad1ee"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['user_fh', 'camp_fh'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1201/1201 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.7818 - auc: 0.9858\n",
      "Epoch 1: val_accuracy improved from -inf to 0.76569, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.49668, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "1201/1201 [==============================] - 33s 22ms/step - loss: 0.4527 - accuracy: 0.7818 - auc: 0.9858 - val_loss: 0.4967 - val_accuracy: 0.7657 - val_auc: 0.9834\n",
      "Epoch 2/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8327 - auc: 0.9905\n",
      "Epoch 2: val_accuracy improved from 0.76569 to 0.82674, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.49668 to 0.37814, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "1201/1201 [==============================] - 30s 23ms/step - loss: 0.3691 - accuracy: 0.8327 - auc: 0.9905 - val_loss: 0.3781 - val_accuracy: 0.8267 - val_auc: 0.9900\n",
      "Epoch 3/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.8406 - auc: 0.9914\n",
      "Epoch 3: val_accuracy improved from 0.82674 to 0.83591, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.37814 to 0.36089, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "1201/1201 [==============================] - 29s 23ms/step - loss: 0.3526 - accuracy: 0.8406 - auc: 0.9914 - val_loss: 0.3609 - val_accuracy: 0.8359 - val_auc: 0.9910\n",
      "Epoch 4/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.8451 - auc: 0.9919\n",
      "Epoch 4: val_accuracy improved from 0.83591 to 0.83741, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.36089 to 0.35924, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "1201/1201 [==============================] - 29s 23ms/step - loss: 0.3424 - accuracy: 0.8451 - auc: 0.9919 - val_loss: 0.3592 - val_accuracy: 0.8374 - val_auc: 0.9910\n",
      "Epoch 5/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.8474 - auc: 0.9922\n",
      "Epoch 5: val_accuracy did not improve from 0.83741\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.35924\n",
      "1201/1201 [==============================] - 28s 22ms/step - loss: 0.3364 - accuracy: 0.8473 - auc: 0.9922 - val_loss: 0.3603 - val_accuracy: 0.8365 - val_auc: 0.9909\n",
      "Epoch 6/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.8496 - auc: 0.9924\n",
      "Epoch 6: val_accuracy improved from 0.83741 to 0.83928, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel_checkpoint.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.35924 to 0.35621, saving model to ./deep_and_wide/NN_checkpoint/W&D/wdmodel.h5\n",
      "1201/1201 [==============================] - 29s 23ms/step - loss: 0.3311 - accuracy: 0.8496 - auc: 0.9924 - val_loss: 0.3562 - val_accuracy: 0.8393 - val_auc: 0.9912\n",
      "Epoch 7/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8512 - auc: 0.9926\n",
      "Epoch 7: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3266 - accuracy: 0.8512 - auc: 0.9926 - val_loss: 0.3581 - val_accuracy: 0.8387 - val_auc: 0.9910\n",
      "Epoch 8/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8525 - auc: 0.9927\n",
      "Epoch 8: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3237 - accuracy: 0.8525 - auc: 0.9927 - val_loss: 0.3613 - val_accuracy: 0.8384 - val_auc: 0.9908\n",
      "Epoch 9/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8534 - auc: 0.9929\n",
      "Epoch 9: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3207 - accuracy: 0.8534 - auc: 0.9929 - val_loss: 0.3621 - val_accuracy: 0.8380 - val_auc: 0.9908\n",
      "Epoch 10/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.8539 - auc: 0.9930\n",
      "Epoch 10: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3184 - accuracy: 0.8539 - auc: 0.9930 - val_loss: 0.3623 - val_accuracy: 0.8376 - val_auc: 0.9908\n",
      "Epoch 11/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8545 - auc: 0.9930\n",
      "Epoch 11: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3165 - accuracy: 0.8545 - auc: 0.9930 - val_loss: 0.3663 - val_accuracy: 0.8370 - val_auc: 0.9905\n",
      "Epoch 12/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8547 - auc: 0.9931\n",
      "Epoch 12: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3155 - accuracy: 0.8547 - auc: 0.9931 - val_loss: 0.3688 - val_accuracy: 0.8375 - val_auc: 0.9902\n",
      "Epoch 13/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8556 - auc: 0.9932\n",
      "Epoch 13: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3130 - accuracy: 0.8556 - auc: 0.9932 - val_loss: 0.3713 - val_accuracy: 0.8363 - val_auc: 0.9900\n",
      "Epoch 14/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8561 - auc: 0.9932\n",
      "Epoch 14: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3120 - accuracy: 0.8561 - auc: 0.9932 - val_loss: 0.3715 - val_accuracy: 0.8355 - val_auc: 0.9900\n",
      "Epoch 15/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8562 - auc: 0.9933\n",
      "Epoch 15: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3104 - accuracy: 0.8562 - auc: 0.9933 - val_loss: 0.3748 - val_accuracy: 0.8347 - val_auc: 0.9898\n",
      "Epoch 16/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3102 - accuracy: 0.8561 - auc: 0.9933\n",
      "Epoch 16: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3102 - accuracy: 0.8561 - auc: 0.9933 - val_loss: 0.3753 - val_accuracy: 0.8349 - val_auc: 0.9897\n",
      "Epoch 17/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.8569 - auc: 0.9933\n",
      "Epoch 17: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3084 - accuracy: 0.8569 - auc: 0.9933 - val_loss: 0.3803 - val_accuracy: 0.8336 - val_auc: 0.9894\n",
      "Epoch 18/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8571 - auc: 0.9933\n",
      "Epoch 18: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3077 - accuracy: 0.8571 - auc: 0.9933 - val_loss: 0.3822 - val_accuracy: 0.8343 - val_auc: 0.9891\n",
      "Epoch 19/300\n",
      "1201/1201 [==============================] - ETA: 0s - loss: 0.3069 - accuracy: 0.8570 - auc: 0.9934\n",
      "Epoch 19: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3069 - accuracy: 0.8570 - auc: 0.9934 - val_loss: 0.3841 - val_accuracy: 0.8331 - val_auc: 0.9889\n",
      "Epoch 20/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8575 - auc: 0.9934\n",
      "Epoch 20: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3063 - accuracy: 0.8575 - auc: 0.9934 - val_loss: 0.3860 - val_accuracy: 0.8347 - val_auc: 0.9886\n",
      "Epoch 21/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8577 - auc: 0.9934\n",
      "Epoch 21: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3050 - accuracy: 0.8577 - auc: 0.9934 - val_loss: 0.3886 - val_accuracy: 0.8335 - val_auc: 0.9884\n",
      "Epoch 22/300\n",
      "1199/1201 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.8581 - auc: 0.9935\n",
      "Epoch 22: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3044 - accuracy: 0.8581 - auc: 0.9935 - val_loss: 0.3907 - val_accuracy: 0.8331 - val_auc: 0.9883\n",
      "Epoch 23/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8584 - auc: 0.9935\n",
      "Epoch 23: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 0.3037 - accuracy: 0.8584 - auc: 0.9935 - val_loss: 0.3945 - val_accuracy: 0.8323 - val_auc: 0.9878\n",
      "Epoch 24/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8582 - auc: 0.9935\n",
      "Epoch 24: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3032 - accuracy: 0.8582 - auc: 0.9935 - val_loss: 0.3965 - val_accuracy: 0.8321 - val_auc: 0.9876\n",
      "Epoch 25/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.8585 - auc: 0.9935\n",
      "Epoch 25: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3025 - accuracy: 0.8585 - auc: 0.9935 - val_loss: 0.3996 - val_accuracy: 0.8304 - val_auc: 0.9874\n",
      "Epoch 26/300\n",
      "1200/1201 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8580 - auc: 0.9935\n",
      "Epoch 26: val_accuracy did not improve from 0.83928\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.35621\n",
      "1201/1201 [==============================] - 27s 21ms/step - loss: 0.3029 - accuracy: 0.8580 - auc: 0.9935 - val_loss: 0.4006 - val_accuracy: 0.8329 - val_auc: 0.9872\n",
      "Epoch 26: early stopping\n"
     ]
    }
   ],
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    wd_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=wdmodel_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H = wdmodel.fit(train_dl,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs,\n",
    "                    validation_data=val_dl,\n",
    "                    shuffle=False,\n",
    "                    validation_batch_size=batch_size,\n",
    "                    callbacks=[wd_es, wd_mc, wd_m])\n",
    "else:\n",
    "    wdmodel = tf.keras.models.load_model(wdmodel_path)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    },
    "id": "kSURd1guvjUW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660228068978,
     "user_tz": -120,
     "elapsed": 878957,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "e0259966-c6ba-42de-fc6e-fcd40c24356d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1201/1201 [==============================] - 18s 14ms/step - loss: 0.2961 - accuracy: 0.8613 - auc: 0.9938\n",
      "401/401 [==============================] - 6s 14ms/step - loss: 0.4006 - accuracy: 0.8329 - auc: 0.9872\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.4037 - accuracy: 0.8315 - auc: 0.9870\n",
      "\n",
      "[INFO] On Training Set:\n",
      "[0.2960934042930603, 0.8612719178199768, 0.9938235878944397]\n",
      "\n",
      "[INFO] On Validation Set:\n",
      "[0.40061330795288086, 0.8328808546066284, 0.9872100949287415]\n",
      "\n",
      "[INFO] On Test Set:\n",
      "[0.4036564528942108, 0.8314800262451172, 0.9869565963745117]\n"
     ]
    }
   ],
   "source": [
    "eval_wdmodel_train = wdmodel.evaluate(train_dl, batch_size=batch_size)\n",
    "eval_wdmodel_val = wdmodel.evaluate(val_dl, batch_size=batch_size)\n",
    "eval_wdmodel_test = wdmodel.evaluate(test_dl, batch_size=batch_size)\n",
    "# Print the results\n",
    "print(\"\\n[INFO] On Training Set:\")\n",
    "print(eval_wdmodel_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(eval_wdmodel_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(eval_wdmodel_test)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    },
    "id": "QdJpnYa4vjUW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1660228141721,
     "user_tz": -120,
     "elapsed": 34069,
     "user": {
      "displayName": "shubhaditya burela",
      "userId": "17820068407271373572"
     }
    },
    "outputId": "9cbe82ce-db76-43c9-a6be-6f84b91a23cd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BAYESIAN WIDE & DEEP MODEL"
   ],
   "metadata": {
    "id": "gdYM_UD2vjUW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bmodel, bmodel_path, b_es, b_mc = build_bayesian_model(feature_column_dict,\n",
    "                                                       inputs,\n",
    "                                                       bayesian_dir)\n",
    "bmodel.summary()  # To display the architecture"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bqCu6ZoLvjUX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "again_training = True\n",
    "if again_training:\n",
    "    # create callback for model saving\n",
    "    b_m = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=bmodel_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    H = bmodel.fit(train_dl,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=n_epochs,\n",
    "                   validation_data=val_dl,\n",
    "                   shuffle=False,\n",
    "                   validation_batch_size=batch_size,\n",
    "                   callbacks=[b_es, b_mc, b_m])\n",
    "else:\n",
    "    bmodel = tf.keras.models.load_model(bmodel_path)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% Training already done - Just load the model!\n"
    },
    "id": "HC4hohR2vjUX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts_train, ucb_train = evaluate_bandit(bmodel, train_dl)\n",
    "ts_val, ucb_val = evaluate_bandit(bmodel, val_dl)\n",
    "ts_test, ucb_test = evaluate_bandit(bmodel, test_dl)\n",
    "# Print the results\n",
    "print(\"\\nUCB\\n[INFO] On Training Set:\")\n",
    "print(ucb_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(ucb_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(ucb_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nThompson Sampling\\n[INFO] On Training Set:\")\n",
    "print(ts_train)\n",
    "print(\"\\n[INFO] On Validation Set:\")\n",
    "print(ts_val)\n",
    "print(\"\\n[INFO] On Test Set:\")\n",
    "print(ts_test)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% Generate predictions on train, val & test set\n"
    },
    "id": "K-hvOJsWvjUX"
   }
  }
 ]
}